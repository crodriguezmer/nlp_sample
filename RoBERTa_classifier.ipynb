{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5a7d6dc6-7d7d-41d2-bd03-e8da77a9564b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "from transformers import RobertaTokenizerFast, RobertaForSequenceClassification, Trainer, TrainingArguments, TextClassificationPipeline\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "import os\n",
    "import json\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "basedir = os.path.abspath('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20bbaa5-430f-4d3a-9d70-3f46fd9eaf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c3a4ebf2c9a3143f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/christian/.cache/huggingface/datasets/csv/default-c3a4ebf2c9a3143f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00877523422241211,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Downloading data files",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcb541e56c284447969ca65bf6913d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006795167922973633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "Extracting data files",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50fb87327bcd45b39bec1dafdd143cb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.025536060333251953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": null,
       "unit": " tables",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/christian/.cache/huggingface/datasets/csv/default-c3a4ebf2c9a3143f/0.0.0/652c3096f041ee27b04d2232d41f10547a8fecda3e284a79a0ec4053c916ef7a. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006563663482666016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "982dbdff66ee4152b08d609a13516bce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "file = 'roberta_data.csv'\n",
    "dataset = datasets.load_dataset(\"csv\", data_files=file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c51a30a6-1120-4554-a50a-869cf0073257",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('roberta_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc10d89-e25a-49f4-9ffc-ddeb9e12866e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25000</th>\n",
       "      <td>Missense mutations in TUBB3, the gene that enc...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25001</th>\n",
       "      <td>For resource-constrained IoT systems, data col...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25002</th>\n",
       "      <td>Objectives\\r\\nPrevious attempts at meta-analys...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25003</th>\n",
       "      <td>Increasing attention has been paid to the role...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25004</th>\n",
       "      <td>OBJECTIVES: The severe forms of hypertriglycer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>Strong-gravitational lens systems with quadrup...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>The terminal complement-inhibitor eculizumab h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>Electrical winding faults, namely stator short...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>The advent of the clinically approved drug cis...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>Amyotrophic lateral sclerosis (ALS) is charact...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  label\n",
       "25000  Missense mutations in TUBB3, the gene that enc...      1\n",
       "25001  For resource-constrained IoT systems, data col...      1\n",
       "25002  Objectives\\r\\nPrevious attempts at meta-analys...      1\n",
       "25003  Increasing attention has been paid to the role...      1\n",
       "25004  OBJECTIVES: The severe forms of hypertriglycer...      1\n",
       "...                                                  ...    ...\n",
       "49995  Strong-gravitational lens systems with quadrup...      1\n",
       "49996  The terminal complement-inhibitor eculizumab h...      1\n",
       "49997  Electrical winding faults, namely stator short...      1\n",
       "49998  The advent of the clinically approved drug cis...      1\n",
       "49999  Amyotrophic lateral sclerosis (ALS) is charact...      1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.label==1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3051575-bdea-4162-9b08-78638bcccdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_dataset = dataset['train'].train_test_split(test_size=0.2)\n",
    "train_data = split_dataset['train']\n",
    "test_data = split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "86108dc5-23dd-49db-a0b9-a0c5187511f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# load model and tokenizer and define length of the text sequence\n",
    "model = RobertaForSequenceClassification.from_pretrained('roberta-base',  num_labels=2)\n",
    "tokenizer = RobertaTokenizerFast.from_pretrained('roberta-base', max_length = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91b6151b-c57a-4af8-b57f-30abf1313043",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.016012907028198242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f352d13693458db0a5a511851ad982",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006868839263916016,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 25,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 1,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f9d4bfd28842d3b932ec7b51299e77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# define a function that will tokenize the model, and will return the relevant inputs for the model\n",
    "def tokenization(batched_text):\n",
    "    return tokenizer(batched_text['text'], padding = True, truncation=True)\n",
    "\n",
    "\n",
    "train_data = train_data.map(tokenization, batched = True, batch_size = len(train_data))\n",
    "test_data = test_data.map(tokenization, batched = True, batch_size = len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c5450f9-0c15-4b63-a9ee-3dd755b9cdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = basedir + '/results',\n",
    "    num_train_epochs = 3,\n",
    "    per_device_train_batch_size = 4,\n",
    "    gradient_accumulation_steps = 16,    \n",
    "    per_device_eval_batch_size = 8,\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    save_strategy = \"epoch\",\n",
    "    disable_tqdm = False, \n",
    "    load_best_model_at_end = True,\n",
    "    warmup_steps = 500,\n",
    "    weight_decay = 0.01,\n",
    "    logging_steps = 8,\n",
    "    logging_dir = basedir + '/results',\n",
    "    dataloader_num_workers = 8,\n",
    "    run_name = 'roberta-classification'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0735d2a3-d98d-4de6-ad3e-8c5949fed578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the trainer class and check for available devices\n",
    "trainer = Trainer(\n",
    "    train_dataset=train_data,\n",
    "    eval_dataset=test_data,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3a37147-f3f4-4714-b05b-da713b813c43",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/Users/christian/Documents/openalex-snapshot/ml/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 40000\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 64\n",
      "  Gradient Accumulation steps = 16\n",
      "  Total optimization steps = 1875\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mcrodriguezmer\u001b[0m (\u001b[33mology_ml\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/christian/Documents/openalex-snapshot/wandb/run-20220908_093104-36718ini</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/ology_ml/huggingface/runs/36718ini\" target=\"_blank\">roberta-classification</a></strong> to <a href=\"https://wandb.ai/ology_ml/huggingface\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1875' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1875/1875 35:45:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.356100</td>\n",
       "      <td>0.362228</td>\n",
       "      <td>0.847100</td>\n",
       "      <td>0.848749</td>\n",
       "      <td>0.831718</td>\n",
       "      <td>0.866492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.293700</td>\n",
       "      <td>0.339273</td>\n",
       "      <td>0.862500</td>\n",
       "      <td>0.870393</td>\n",
       "      <td>0.816013</td>\n",
       "      <td>0.932539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.187700</td>\n",
       "      <td>0.348097</td>\n",
       "      <td>0.865600</td>\n",
       "      <td>0.870943</td>\n",
       "      <td>0.830130</td>\n",
       "      <td>0.915977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /Users/christian/Documents/openalex-snapshot/results/checkpoint-625\n",
      "Configuration saved in /Users/christian/Documents/openalex-snapshot/results/checkpoint-625/config.json\n",
      "Model weights saved in /Users/christian/Documents/openalex-snapshot/results/checkpoint-625/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /Users/christian/Documents/openalex-snapshot/results/checkpoint-1250\n",
      "Configuration saved in /Users/christian/Documents/openalex-snapshot/results/checkpoint-1250/config.json\n",
      "Model weights saved in /Users/christian/Documents/openalex-snapshot/results/checkpoint-1250/pytorch_model.bin\n",
      "The following columns in the evaluation set don't have a corresponding argument in `RobertaForSequenceClassification.forward` and have been ignored: text. If text are not expected by `RobertaForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 10000\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to /Users/christian/Documents/openalex-snapshot/results/checkpoint-1875\n",
      "Configuration saved in /Users/christian/Documents/openalex-snapshot/results/checkpoint-1875/config.json\n",
      "Model weights saved in /Users/christian/Documents/openalex-snapshot/results/checkpoint-1875/pytorch_model.bin\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from /Users/christian/Documents/openalex-snapshot/results/checkpoint-1250 (score: 0.33927321434020996).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1875, training_loss=0.3177709925969442, metrics={'train_runtime': 128769.8853, 'train_samples_per_second': 0.932, 'train_steps_per_second': 0.015, 'total_flos': 3.15733266432e+16, 'train_loss': 0.3177709925969442, 'epoch': 3.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a980f4f3-80f3-495b-81f0-78cd9250b49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/christian/Documents/openalex-snapshot'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85d2975b-6122-4fff-bc64-e7c585e34e5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in ./save_pretrained/tokenizer_config.json\n",
      "Special tokens file saved in ./save_pretrained/special_tokens_map.json\n",
      "Configuration saved in ./save_pretrained/config.json\n",
      "Model weights saved in ./save_pretrained/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "save_directory = \"./save_pretrained\"\n",
    "tokenizer.save_pretrained(save_directory)\n",
    "model.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9978540d-7f67-4c70-817a-a93e3fa01bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./save_pretrained/config.json\n",
      "Model config RobertaConfig {\n",
      "  \"_name_or_path\": \"roberta-base\",\n",
      "  \"architectures\": [\n",
      "    \"RobertaForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"model_type\": \"roberta\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.21.2\",\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "loading weights file ./save_pretrained/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing RobertaForSequenceClassification.\n",
      "\n",
      "All the weights of RobertaForSequenceClassification were initialized from the model checkpoint at ./save_pretrained.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use RobertaForSequenceClassification for predictions without further training.\n",
      "Didn't find file ./save_pretrained/added_tokens.json. We won't load it.\n",
      "loading file ./save_pretrained/vocab.json\n",
      "loading file ./save_pretrained/merges.txt\n",
      "loading file ./save_pretrained/tokenizer.json\n",
      "loading file None\n",
      "loading file ./save_pretrained/special_tokens_map.json\n",
      "loading file ./save_pretrained/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "pt_model = RobertaForSequenceClassification.from_pretrained(save_directory)\n",
    "pt_tokenizer = RobertaTokenizerFast.from_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3f1d9f9b-0f11-4d00-bd58-736d364a6d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = TextClassificationPipeline(model=model, tokenizer=tokenizer, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0df8f9c2-8f2e-49c0-a0f7-d3dee563ef94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flow microcalorimetric titrations of calmodulin with melittin at 25 degrees C revealed that the formation of the high-affinity one-to-one complex in the presence of Ca2+ (Comte, M., Maulet, Y., and Cox, J. A. (1983) Biochem, J. 209, 269-272) is entirely entropy driven (delta H0 = 30.3 kJ X mol-1; delta S0 = 275 J X K-1 X mol-1). Neither the proton nor the Mg2+ concentrations have any significant effect on the strength of the complex. In the absence of Ca2+, a nonspecific calmodulin-(melittin)n complex is formed; the latter is predominantly entropy driven, accompanied by a significant uptake of protons and fully antagonized by Mg2+. Enthalpy titrations of metal-free calmodulin with Ca2+ in the presence of an equimolar amount of melittin were carried out at pH 7.0 in two buffers of different protonation enthalpy. The enthalpy and proton release profiles indicate that: protons, absorbed by the nonspecific calmodulin-melittin complex, are released upon binding of the first Ca2+; Ca2+ binding to the high-affinity configuration of the calmodulin-melittin complex displays an affinity constant greater than or equal to 10(7) M-1, i.e. 2 orders of magnitude higher than that of free calmodulin; the latter is even more entropy driven (delta H0 = 7.2 kJ X site-1; delta S0 = 158 J X K-1 X site-1) than binding to free calmodulin (delta H0 = 4.7 kJ X site-1; delta S0 = 112 J X K-1 X site-1), thus underlining the importance of hydrophobic forces in the free energy coupling involved in the ternary complex. \n",
      "[[{'label': 'LABEL_1', 'score': 0.9005618691444397}, {'label': 'LABEL_0', 'score': 0.09943809360265732}]]\n",
      "\n",
      "\n",
      "This study aims to investigate the effect of foreign exchange rate and trade policies on crop exports in Iran. Regarding foreign exchange policies, the exchange rate volatility index and the dummy variable of currency uniformity policy were considered, while economic openness index as well as the trade bias coefficient index was studied for business policies. Logarithmic model of export supply function based on the ARDL and error correction model were used to estimate long- and short-term relations between variables. The research data were collected from the websites of the International Trade Statistics Database and the Central Bank of Iran from 1997 to 2014. The results showed that variables of commercial deviation coefficient, real exchange rate, economic openness, domestic product price, stone fruits export value in previous years and the dummy variable of currency uniformity policy all had a positive effect, in the long term, on Iranian stone fruits export. The coefficient for error correction model equals -0.42, which is quite significant indicating a very high modulation pace to a long-term equilibrium. In addition, the results revealed that, in the event of shock and distortion in balance in each period, 0.58% of the short-term imbalance has been adjusted to achieve long-term equilibrium.JEL Classification: Q17, Q18, F13, F17, F39 \n",
      "[[{'label': 'LABEL_0', 'score': 0.9922653436660767}, {'label': 'LABEL_1', 'score': 0.00773468567058444}]]\n",
      "\n",
      "\n",
      "Brain-derived neurotrophic factor (BDNF) is a critical growth factor involved in the maturation of the CNS, including neuronal morphology and synapse refinement. Herein, we demonstrate astrocytes express high levels of BDNF’s receptor, TrkB (in the top 20 of protein-coding transcripts), with nearly exclusive expression of the truncated isoform, TrkB.T1, which peaks in expression during astrocyte morphological maturation. Using a novel culture paradigm, we show that astrocyte morphological complexity is increased in the presence of BDNF and is dependent upon BDNF/TrkB.T1 signaling. Deletion of TrkB.T1, globally and astrocyte-specifically, in mice revealed morphologically immature astrocytes with significantly reduced volume, as well as dysregulated expression of perisynaptic genes associated with mature astrocyte function. Indicating a role for functional astrocyte maturation via BDNF/TrkB.T1 signaling, TrkB.T1 KO astrocytes do not support normal excitatory synaptogenesis or function. These data suggest a significant role for BDNF/TrkB.T1 signaling in astrocyte morphological maturation, a critical process for CNS development. \n",
      "[[{'label': 'LABEL_1', 'score': 0.9819157123565674}, {'label': 'LABEL_0', 'score': 0.018084244802594185}]]\n",
      "\n",
      "\n",
      "The PI3K pathway is genetically altered in excess of 70% of breast cancers, largely through PIK3CA mutation and HER2 amplification. Preclinical studies have suggested that these subsets of breast cancers are particularly sensitive to PI3K inhibitors; however, the reasons for this heightened sensitivity are mainly unknown. We investigated the signaling effects of PI3K inhibition in PIK3CA mutant and HER2 amplified breast cancers using PI3K inhibitors currently in clinical trials. Unexpectedly, we found that in PIK3CA mutant and HER2 amplified breast cancers sensitive to PI3K inhibitors, PI3K inhibition led to a rapid suppression of Rac1/p21-activated kinase (PAK)/protein kinase C-RAF (C-RAF)/ protein kinase MEK (MEK)/ERK signaling that did not involve RAS. Furthermore, PI3K inhibition led to an ERK-dependent up-regulation of the proapoptotic protein, BIM, followed by induction of apoptosis. Expression of a constitutively active form of Rac1 in these breast cancer models blocked PI3Ki-induced down-regulation of ERK phosphorylation, apoptosis, and mitigated PI3K inhibitor sensitivity in vivo. In contrast, protein kinase AKT inhibitors failed to block MEK/ERK signaling, did not up-regulate BIM, and failed to induce apoptosis. Finally, we identified phosphatidylinositol 3,4,5-trisphosphate-dependent Rac exchanger 1 (P-Rex1) as the PI(3,4,5)P3-dependent guanine exchange factor for Rac1 responsible for regulation of the Rac1/C-RAF/MEK/ERK pathway in these cells. The expression level of P-Rex1 correlates with sensitivity to PI3K inhibitors in these breast cancer cell lines. Thus, PI3K inhibitors have enhanced activity in PIK3CA mutant and HER2 amplified breast cancers in which PI3K inhibition down-regulates both the AKT and Rac1/ERK pathways. In addition, P-Rex1 may serve as a biomarker to predict response to single-agent PI3K inhibitors within this subset of breast cancers. \n",
      "[[{'label': 'LABEL_1', 'score': 0.9817023277282715}, {'label': 'LABEL_0', 'score': 0.018297653645277023}]]\n",
      "\n",
      "\n",
      "Abstract   Iron-based alloys are now being considered as plasma-facing materials for the first wall of future fusion reactors. Therefore, the iron (Fe) and carbon (C) erosion will play a key role in predicting the life-time and viability of reactors with steel walls. In this work, the surface erosion and morphology changes due to deuterium (D) irradiation in pure Fe, Fe with 1% C impurity and the cementite, are studied using molecular dynamics (MD) simulations, varying surface temperature and impact energy. The sputtering yields for both Fe and C were found to increase with incoming energy. In iron carbide, C sputtering was preferential to Fe and the deuterium was mainly trapped as D2 in bubbles, while mostly atomic D was present in Fe and Fe–1%C. The sputtering yields obtained from MD were compared to SDTrimSP yields. At lower impact energies, the sputtering mechanism was of both physical and chemical origin, while at higher energies (>100 eV) the physical sputtering dominated. \n",
      "[[{'label': 'LABEL_1', 'score': 0.9761584401130676}, {'label': 'LABEL_0', 'score': 0.023841559886932373}]]\n",
      "\n",
      "\n",
      "We constrain the coupling between axionlike particles (ALPs) and photons, measured with the superconducting resonant detection circuit of a cryogenic Penning trap. By searching the noise spectrum of our fixed-frequency resonant circuit for peaks caused by dark matter ALPs converting into photons in the strong magnetic field of the Penning-trap magnet, we are able to constrain the coupling of ALPs with masses around $2.7906-2.7914\\,\\textrm{neV/c}^2$ to $g_{a\\gamma}< 1 \\times 10^{-11}\\,\\textrm{GeV}^{-1}$. This is more than one order of magnitude lower than the best laboratory haloscope and approximately 5 times lower than the CERN axion solar telescope (CAST), setting limits in a mass and coupling range which is not constrained by astrophysical observations. Our approach can be extended to many other Penning-trap experiments and has the potential to provide broad limits in the low ALP mass range. \n",
      "[[{'label': 'LABEL_1', 'score': 0.8983450531959534}, {'label': 'LABEL_0', 'score': 0.10165492445230484}]]\n",
      "\n",
      "\n",
      "Radiative cooling is a technology intended to provide cooling using the sky as a heat sink. This technology has been widely studied since 20th century but its research is scattered all over the literature, requiring of a review to gather all information and a state-of-the-art. In the present article, the research has been classified in: (1) radiative cooling background, (2) selective radiative cooling, (3) theoretical approach and numerical simulations, and (4) radiative cooling prototypes. Even though this is a low-grade technology it can dramatically reduce the energy consumption, since it is renewable and requires low energy for its operation. However, new functionalities of the device, apart from radiative cooling, are required for profitable reasons. Some recommendations extracted from the literature to improve the efficiency of radiative cooling are: to use a cover to achieve low temperatures, to use water instead of air as heat-carrier fluid, and to couple the device with heat storage. Finally, further research should be focused in the development of new materials with improved radiative properties, the measurement of incoming infrared atmospheric radiation and/or new technics to predict it, and the evaluation of new device concepts. \n",
      "[[{'label': 'LABEL_1', 'score': 0.9062589406967163}, {'label': 'LABEL_0', 'score': 0.09374114125967026}]]\n",
      "\n",
      "\n",
      "Qualitative studies in Surgery are important because they contextualize the previously missing social facets of the surgical narrative and inquire into the crucial issues of quality of life/well-being, gender and other discriminations and biases faced by surgeons and patients, surgical education/training, mental issues and burnout, etc. This has resulted in an increasing trend of qualitative studies in surgery. Authors, editors and journals have to ensure that the principles of scientific rigour in qualitative research are followed; otherwise, the answers will not be valid, thus rendering the whole exercise futile. More studies, addressing these fascinating ‘social’ facets of surgery, are needed. \n",
      "[[{'label': 'LABEL_0', 'score': 0.9906249642372131}, {'label': 'LABEL_1', 'score': 0.009375117719173431}]]\n",
      "\n",
      "\n",
      "Summary\n",
      "\n",
      "\n",
      "In January 2014, approximately 9 months following the initial detection of porcine epidemic diarrhea (PED) in the USA, the first case of PED was confirmed in a swine herd in south-western Ontario. A follow-up epidemiological investigation carried out on the initial and 10 subsequent Ontario PED cases pointed to feed as a common risk factor. As a result, several lots of feed and spray-dried porcine plasma (SDPP) used as a feed supplement were tested for the presence of PEDV genome by real-time RT-PCR assay. Several of these tested positive, supporting the notion that contaminated feed may have been responsible for the introduction of PEDV into Canada. These findings led us to conduct a bioassay experiment in which three PEDV-positive SDPP samples (from a single lot) and two PEDV-positive feed samples supplemented with this SDPP were used to orally inoculate 3-week-old piglets. Although the feed-inoculated piglets did not show any significant excretion of PEDV, the SDPP-inoculated piglets shed PEDV at a relatively high level for ≥9 days. Despite the fact that the tested PEDV genome positive feed did not result in obvious piglet infection in our bioassay experiment, contaminated feed cannot be ruled out as a likely source of this introduction in the field where many other variables may play a contributing role. \n",
      "[[{'label': 'LABEL_1', 'score': 0.6451302766799927}, {'label': 'LABEL_0', 'score': 0.35486969351768494}]]\n",
      "\n",
      "\n",
      "Objectives: We sought to describe the travel distance and time, cost, and wait time to first available appointment for North Carolina (NC) students seeking a medication abortion at the closest abortion-providing facility. \n",
      "[[{'label': 'LABEL_0', 'score': 0.926620364189148}, {'label': 'LABEL_1', 'score': 0.07337966561317444}]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for abstract in test_data['text'][:10]:\n",
    "    print(abstract)\n",
    "    print(pipe(abstract))\n",
    "    print('\\n')\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
